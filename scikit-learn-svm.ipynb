{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation  \n",
    "=============\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for <font color = 'green'>classification, regression and outliers detection </font>.\n",
    "\n",
    "<font color = 'red' size = '4pt'> Advantage: </font> \n",
    "\n",
    "1>  Uses a subset of training points in the decision process (called support vectors), so it is memory efficient\n",
    "\n",
    "2> Versatile: Different kernel function can be specified for the decision function\n",
    "\n",
    "\n",
    "<font color = 'red' size = '4pt'> Disadvantage: </font>\n",
    "\n",
    "1> If the number of feature is more than number of samples, avoid overfitting by choosing kernel function and regularizing term is crucial\n",
    "\n",
    "2> SVM do not directly provide probability estimates. \n",
    "\n",
    "\n",
    "How It works\n",
    "===========\n",
    "\n",
    "<font color = 'red'> Scenario1: </font> Select the hyper-plane which segregates the two classes better. In this scenario, hyper-plane “B” has excellently performed this job\n",
    "![title](resources/SVM_1.webp)\n",
    "\n",
    "\n",
    "\n",
    "<font color = 'red'> Scenario2: </font> Maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let’s look at the below snapshot:\n",
    "If you see the below picture.\n",
    "![title](resources/SVM_2.webp)\n",
    "\n",
    "Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.\n",
    "\n",
    "\n",
    "<font color = 'red'> Scenario3: </font> Classification has higher priority over margin. SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. In the below picture, even though B has higher margin compared to A, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.\n",
    "![title](resources/SVM_3.webp)\n",
    "\n",
    "\n",
    "\n",
    "<font color = 'red'> Scenario4: </font> Ignore outliers. SVM has a feature to ignore outliers and find the hyper-plane that has maximum margin. Hence, we can say, SVM is robust to outliers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color = 'red'> Scenario5: </font> Non linear plane. SVM introduce additional feature.\n",
    "\n",
    "![title](resources/SVM_5.webp)\n",
    "\n",
    "\n",
    "SVM can solve this problem. Easily! It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let’s plot the data points on axis x and z:\n",
    "\n",
    "\n",
    "![title](resources/SVM_6.webp)\n",
    "\n",
    "NB vs SVM\n",
    "=========\n",
    "Naive Bayes is great for text--it’s faster and generally gives better performance than an SVM for this particular problem. Of course, there are plenty of other problems where an SVM might work better. \n",
    "In addition to picking your algorithm, depending on which one you try, there are parameter tunes to worry about as well, and the possibility of overfitting (especially if you don’t have lots of training data).\n",
    "\n",
    "Our general suggestion is to try a few different algorithms for each problem. GridCV, a great sklearn tool can find an optimal parameter tune almost automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]] # Train data\n",
    "y = [0, 1] # Label data\n",
    "clf = svm.SVC() # We are using support vector classifier (SVC)\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]]) # This point belongs to label 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[0., 0.5]]) # This point belongs to label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udacity SVM\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\") # you can either import svm and then do svm.svc directly import SVC\n",
    "\n",
    "\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "def submitAccuracy():\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versatile Kernel: \n",
    "=============\n",
    "Different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "As we can see that call can accept certain number of parameter\n",
    "\n",
    "\n",
    "<font color='green'>class sklearn.svm.SVC</font> (<font color='red'>C=1.0</font>, <font color='red'>kernel=’rbf’</font>, degree=3, <font color='red'> gamma=’auto’</font>, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "\n",
    "Out of all these parameters, kernel, C, and gamma is the most important parameter.\n",
    "\n",
    "We can specify \"kernel\" as below\n",
    "\n",
    "kernel : string, optional (<font color='red'>default=’rbf’</font>)\n",
    "\n",
    "Specifies the kernel type to be used in the algorithm. It must be one of <font color='green'>‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable</font> . If none is given, ‘rbf’ will be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF SVM parameters(see example svm_rbf_parameter)\n",
    "============================================\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "\n",
    "This example illustrates the effect of the parameters <font color='green'>gamma and C </font> of the Radial Basis Function (RBF) kernel SVM.\n",
    "\n",
    "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low value of 'gamma' = training points that are far from decision boundary has more influence\n",
    "\n",
    "High Value of 'gamma' = training points that are close from decision boundary has more influence\n",
    "\n",
    "High value of 'c' = incorporation of more traing set \n",
    "\n",
    "Low value of 'c' = more smooth boundary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
