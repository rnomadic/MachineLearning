{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"python-spark-xgb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use data from the [Titanic](https://www.kaggle.com/c/titanic/data): Machine learning from disaster one of the many Kaggle competitions.\n",
    "\n",
    "### Step 1: Download or build the XGBoost jars\n",
    "The python code will need two scala jars dependencies in order to work. I downloaded them directly from maven:\n",
    "\n",
    "1. [xgboost4j](https://mvnrepository.com/artifact/ml.dmlc/xgboost4j/0.72)\n",
    "2. [xgboost4j-spark](https://mvnrepository.com/artifact/ml.dmlc/xgboost4j-spark/0.72)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add the custom XGBoost jars to the Spark app\n",
    "Before starting Spark we need to add the jars we previously downloaded. We can do this using the --jars flag:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Integrate PySpark into the Jupyther notebook\n",
    "Easiest way to make PySpark available is using the findspark package:\n",
    "\n",
    "\n",
    "### Please see the running instruction for more detail of packages that needs to be installed on OSX with Anconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar pyspark-shell'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Start the spark session\n",
    "We are now ready to start the spark session. We are creating a spark app that will run locally and will use as many threads as there are cores using local[*] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PySpark XGBOOST Titanic\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Download the XGBoost python wrapper\n",
    "You can download the PySpark XGBoost code from [here](https://github.com/dmlc/xgboost/files/2161553/sparkxgb.zip). This is the interface between the part that we will write and the XGBoost scala implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Add the PySpark XGBoost wrapper code\n",
    "As we have now the spark session, we can add the wrapper code we dowloaded in step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile(\"../resource/sparkxgb.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkxgb import XGBoostEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Defining a schema\n",
    "Next we define a schema of the data we read from the csv. This is usually a better practice than letting spark to infer the schema because it consumes less resources and we have total control over the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "                    [StructField(\"PassengerId\", DoubleType()),\n",
    "                    StructField(\"Survival\", DoubleType()),\n",
    "                    StructField(\"Pclass\", DoubleType()),\n",
    "                    StructField(\"Name\", StringType()),\n",
    "                    StructField(\"Sex\", StringType()),\n",
    "                    StructField(\"Age\", DoubleType()),\n",
    "                    StructField(\"SibSp\", DoubleType()),\n",
    "                    StructField(\"Parch\", DoubleType()),\n",
    "                    StructField(\"Ticket\", StringType()),\n",
    "                    StructField(\"Fare\", DoubleType()),\n",
    "                    StructField(\"Cabin\", StringType()),\n",
    "                    StructField(\"Embarked\", StringType())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Read the csv data into a dataframe\n",
    "We read the csv into a DataFrame, making sure we mention we have a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark\\\n",
    "    .read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(schema)\\\n",
    "    .csv(\"../resource/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survival|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|        1.0|     0.0|   3.0|Braund, Mr. Owen ...|  male|22.0|  1.0|  0.0|A/5 21171|   7.25| null|       S|\n",
      "|        2.0|     1.0|   1.0|Cumings, Mrs. Joh...|female|38.0|  1.0|  0.0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: we also replace null values with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill Nan value with 0\n",
    "df = df_raw.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Convert the nominal values to numeric\n",
    "ML pipelines, is a set of high level APIs build on top of the DataFrames which make it easier to combine multiple algorithms into a single process. The main elements of a pipeline are the **Transformer** and the **Estimator**. The first can represent an algorithm that can transform a DataFrame into another DataFrame, and the latter is an algorithm that can fit on a DataFrame to produce a Transformer .\n",
    "\n",
    "In order to convert the nominal values into numeric ones we need to define aTransformer for each column:\n",
    "\n",
    "Using the StringIndexer to transform the values. For each Transformer I'm defining the input column and the output column that will contain the modified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexIndexer = StringIndexer()\\\n",
    "  .setInputCol(\"Sex\")\\\n",
    "  .setOutputCol(\"SexIndex\")\\\n",
    "  .setHandleInvalid(\"keep\")\n",
    "    \n",
    "cabinIndexer = StringIndexer()\\\n",
    "  .setInputCol(\"Cabin\")\\\n",
    "  .setOutputCol(\"CabinIndex\")\\\n",
    "  .setHandleInvalid(\"keep\")\n",
    "    \n",
    "embarkedIndexer = StringIndexer()\\\n",
    "  .setInputCol(\"Embarked\")\\\n",
    "  .setOutputCol(\"EmbarkedIndex\")\\\n",
    "  .setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Assemble the columns into a feature vector\n",
    "I will use another Transformer to assemble the columns used in the classification by the XGBoost Estimatorinto a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler()\\\n",
    "                    .setInputCols([\"Pclass\", \"SexIndex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"CabinIndex\", \"EmbarkedIndex\"])\\\n",
    "                    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Defining the XGBoostEstimator\n",
    "In this step I'm defining the Estimator that will produce the model. Most of the parameters used here are default:\n",
    "\n",
    "We only define the feature, label (have to match out columns from the DataFrame ) and the new prediction column that contains the output of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBoostEstimator(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Survival\", \n",
    "    predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Building the pipeline and the classifier\n",
    "I can define the actual pipeline and the order of the operations:\n",
    "The input DataFrame will be transformed multiple times and in the end will produce the model trained with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Train the model and predict on new test data\n",
    "I first split the data into train and test, then I fit the model with the train data and finally I see what predictions I have obtained for each passenger:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|PassengerId|prediction|\n",
      "+-----------+----------+\n",
      "|        1.0|       0.0|\n",
      "|        4.0|       1.0|\n",
      "|       14.0|       0.0|\n",
      "|       15.0|       1.0|\n",
      "|       20.0|       1.0|\n",
      "|       28.0|       1.0|\n",
      "|       34.0|       0.0|\n",
      "|       38.0|       0.0|\n",
      "|       50.0|       1.0|\n",
      "|       52.0|       0.0|\n",
      "|       59.0|       1.0|\n",
      "|       60.0|       0.0|\n",
      "|       82.0|       0.0|\n",
      "|       94.0|       0.0|\n",
      "|       96.0|       0.0|\n",
      "|       99.0|       1.0|\n",
      "|      104.0|       0.0|\n",
      "|      105.0|       0.0|\n",
      "|      107.0|       1.0|\n",
      "|      116.0|       0.0|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)\n",
    "model = pipeline.fit(trainDF)\n",
    "model.transform(testDF).select(col(\"PassengerId\"), col(\"prediction\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
