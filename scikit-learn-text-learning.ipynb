{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Documentation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "The <font color='green' size='3pt'>sklearn.feature_extraction</font> module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\n",
    "\n",
    "\n",
    "\n",
    "# Text feature extraction\n",
    "\n",
    "\n",
    "\n",
    "<font color = 'red' size = '4pt'> The Bag of Words representation </font>\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "<font color = 'red'>tokenizing </font>strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "<font color = 'red'>counting </font> the occurrences of tokens in each document.\n",
    "<font color = 'red'>normalizing </font> normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "<font color= 'red'> CountVectorizer </font> implements both tokenization and occurrence counting in a single class:\n",
    "\n",
    "<font color= 'green'> Parameters </font> : http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Letâ€™s use it to tokenize and count the word occurrences of text documents:\"\n",
    "\n",
    "corpus = [\n",
    "        'This is the first document.',\n",
    "        'This is the second second document.',\n",
    "        'And the third one.',\n",
    "        'Is this the first document?',\n",
    "        ]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'text', 'document', 'to', 'analyze']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The default configuration tokenizes the string by extracting words of at least 2 letters.\"\n",
    "\"The specific function that does this step can be requested explicitly:\"\n",
    "\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Each term found by the analyzer during the fit is assigned \"\n",
    "\"a unique integer index corresponding to a column in the resulting matrix.\"\n",
    "\"This interpretation of the columns can be retrieved as follows:\"\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"From above output we know that there is exactly 9 unique word and they are arranged as per the alphabatical order\"\n",
    "\"So the matrix will be of 4X9, because we have 4 sentence\"\n",
    "\"For each sentence: col value will be 1 if the that word is present in that sentence\"\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The converse mapping from feature name to column index \"\n",
    "\"is stored in the vocabulary_ attribute of the vectorizer:\"\n",
    "\n",
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udacity CountVectorizer\n",
    "====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 6)\t3\n",
      "  (1, 3)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 7)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'best',\n",
       " 'car',\n",
       " 'class',\n",
       " 'driving',\n",
       " 'excellent',\n",
       " 'great',\n",
       " 'hi',\n",
       " 'katie',\n",
       " 'late',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'most',\n",
       " 'sebastian',\n",
       " 'self',\n",
       " 'the',\n",
       " 'will']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = \"Hi Katie the self driving car will be late Best Sebastian\"\n",
    "string2 = \"Hi Sebastian the Machine Learning class will be great great great Best Katie\"\n",
    "string3 = \"hi Katie the machine learning class will be most excellent\"\n",
    "\n",
    "email_list = [string1, string2, string3]\n",
    "vector = CountVectorizer()\n",
    "\n",
    "bag_of_words = vector.fit_transform(email_list)\n",
    "\n",
    "print(bag_of_words)\n",
    "\n",
    "vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above O/P we can see that \n",
    "\n",
    "    Lets take example of (1, 6)    3\n",
    "\n",
    "\n",
    "In Document number 1, feature number 6 appears 3 time \n",
    "\n",
    "As we know python index starts at 0, so doc 1 is string2 and feature number 6 is \"great\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vector.vocabulary_.get('great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stopwords and stemming\n",
    "=========================\n",
    "\n",
    "\n",
    " <font color='green' size = '4pt'> See the example in ud120-projects/text-learning/nltk_stopwords.py </font>\n",
    " \n",
    " The order of text processing should be <font color='green' size = '3pt'> 1> stemming 2> bag_of_words </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF repesentation\n",
    "================\n",
    "\n",
    "<font color= 'red' size='3pt'> TF => </font> Term Frequency (similar to bag of words)\n",
    "\n",
    "<font color= 'red' size='3pt'> IDF => </font> Inverse Document Frequency (put higher weigtht on rare words than on common words across all document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words: 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "vectorizer.fit_transform(email_list)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print('Number of different words: {0}'.format(len(feature_names)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clustering of text documents see below.\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
